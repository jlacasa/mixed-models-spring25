---
title: Fundamentals of linear mixed models 
nav: Day 1
topics: Review; Fixed effects versus random effects
---

## Welcome!

- [About me](https://jlacasa.github.io/).
- About you.
- Frequent responses.   

{% include figure.html img="day1/attendees.jpg" alt="Attendees counts" caption="**Figure 1.** Distribution of Departments attending this worksop" width="75%" %}

## Housekeeping  

- We will have relatively low proportion of R code in this workshop. Instead, we will focus on the understanding of the model components. Questions/concerns are more than welcome.   

Schedule:

{% capture text %}
1. Fundamentals of linear mixed models
   Review of linear models & basic concepts of linear mixed models     
3. Modeling data generated by designed experiments
5. Generalized linear mixed models (aka non-normal response) applied to designed experiments  
{% endcapture %}
{% include card.html text=text header="Workshop Overview" %}


## Outline for today

-   **Review on linear models**: Refresh (all-fixed-effects) linear models, and get to mixed-effects models by adding an extra assumption. 
-   **Review on variance-covariance matrices**: this is important because the information of the random effects is stored in the variance-covariance matrix.  
-   **Fixed effects versus random effects**: Where does the information in the fixed effects and random effects go?
-   **Application**: what does this all mean when we want to fit a model to our data?

------

## Review on linear models

### The famous intercept-and-slope linear model

{% include figure.html img="day1/linear_regression_1.jpg" alt="" caption="**Figure 2.** A good example for the intercept-and-slope model: Apple diameter versus time." width="75%" %}

One of the most popular models is the intercept-and-slope model. Why? Because it's so simple and interpretable! Most of us learned this way of writing out the statistical model:

$$y_{i} = \beta_0 + x_{i} \beta_1 + \varepsilon_{i}, \\ \varepsilon_i \sim N(0, \sigma^2),$$  

where $$y_{i}$$ is the observed value for the $$i$$th observation, $$\beta_0$$ is the intercept, $$\beta_1$$ is the slope parameter, and $$\varepsilon_{i}$$ is the difference between the observed ($$y$$) and the expected ($$E(y_i)=\mu_i=\beta_0+x_i\beta_1$$) and that's why we often call it "residual".


Look at the plot above. A farmer decided to measure the diameter or **random apples from random trees** at different points in time.  

Can we fit the model to that data? Let's review the assumptions we make in this model.   
- Linearity  
- Constant variance  
- Independence  
- Normality  


### Same statistical model using distribution notation and matrix notation

There is another way of writing out the intercept-and-slope statistical model above:  

$$y_{i} \sim N(\mu_i, \sigma^2),$$  
$$\mu_i = \beta_0 + x_{i} \beta_1.$$

Also,  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),$$  
$$\boldsymbol{\mu} = \boldsymbol{\beta_0} + x \boldsymbol{\beta_1}.$$

The advantages of writing out statistical models with this type of notation are 

i. It is easier to switch to other distributions (Day 3 of this workshop)  
ii. It is easier to define and understand the variance-covariance, especially in a mixed models scenario!  

For now, let's focus on the normal distribution...  

### Review on variance-covariance matrices  

#### What is variance?  

Random variables are usually described with their properties like the expected value and variance. 
The expected value and variance are the first and second central moments of a distribution, respectively. 
Regardless of the distribution of a random variable $$Y$$, we could calculate its expected value $$E(Y)$$ and variance $$Var(Y)$$.  
The expected value measures the average outcome of $$Y$$.  
The variance measures the dispersion of $$Y$$, i.e. how far the possible outcomes are spread out from their average. 

{% include figure.html img="day1/normal_univariate.png" alt="Univariate Normal distributions" caption="**Figure 3.** Normal distributions" width="75%" %}

**Discuss in the plot above:**    
-   Expected value  
-   Variance  
-   Covariance?

#### What is covariance?  

When two random variables behave similarly. For example, let's take two variables $$y_1$$ and $$y_2$$ that have variances of 1 each, and also a covariance of 0.6 (Figure 4). We can write that out as 
The variance of a random variable is the covariance of a random variable with itself.  

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right).$$

{% include figure.html img="day1/normal_multivariate.jpg" alt="Multivariate Normal distribution" caption="**Figure 4.** $$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right).$$" width="75%" %}

#### Covariance structures:  

Let's assume we have 10 observations of apple diameter. Then,   

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma)$$  

$$
\begin{array}{c c} 
\begin{array}{c c c} 
\mathbf{y} \equiv & 
\begin{array}{c c c c c c c c c c} \text{obs 1}\\ \text{obs 2}\\ \text{obs 3}\\
\text{obs 4}\\ \text{obs 5}\\ \text{obs 6}\\ \text{obs 7}\\ 
\text{obs 8}\\ \text{obs 9}\\ \text{obs 10} \end{array} &
\left[
\begin{array}{c}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
y_5 \\
y_6 \\
y_7 \\
y_8 \\
y_9 \\
y_{10} 
\end{array}
\right]
\end{array}

\begin{array}{c c} 
\Sigma \equiv 
& \begin{array}{c c c c c c c c c c} \text{obs 1}& \text{obs 2}& \text{obs 3}&
\text{obs 4}& \text{obs 5}& \text{obs 6}& \text{obs 7}& 
\text{obs 8}& \text{obs 9}& \text{obs 10}  \\ \end{array} \\
\begin{array}{c c c c c c c c c c} \text{obs 1}\\ \text{obs 2}\\ \text{obs 3}\\
\text{obs 4}\\ \text{obs 5}\\ \text{obs 6}\\ \text{obs 7}\\ 
\text{obs 8}\\ \text{obs 9}\\ \text{obs 10} \end{array} &
\left[
\begin{array}{c c c}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{array}
\right]
\end{array}

\end{array}
$$


$$\mathbf{y}_{n \times 1} \sim N(\boldsymbol{\mu}_{n \times 1}, \sigma^2\mathbf{I}_{n \times n}),  $$ where $$\mathbf{I}_{n \times n}$$ is the identity matrix with $$n$$ rows and $$n$$ columns. Suppose $$n = 4$$, then $$\mathbf{I}_{4 \times 4} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1  \end{bmatrix}$$.

$$
\begin{array}{c c} 
& \begin{array}{c c c} a & b &c \\ \end{array} \\
\begin{array}{c c c}p\\q\\r \end{array} &
\left[
\begin{array}{c c c}
.1 & .1 & 0 \\
.4 & 1 & 0 \\
.8 & 0 & .4
\end{array}
\right]
\end{array}
$$

- Short demonstration of different variance-covariance functions using R. [[see R code](#)]  


## What are mixed models anyways?

Mixed models (also called "multilevel models") model some of their parameters (i.e., regression coefficients) with probability distributions.

### Fixed effects versus random effects  


#### Method of estimation  

- REML is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased.
- Why is the unbiased estimation of variance components so important?  
  - Relationship between variance estimates, standard error, confidence intervals, t-tests, type I error.


## Applied example  
**Example:** Randomized complete block design.  

-   Field experiment at Colby, KS.  
-   One treatment factor (treatment structure).  
-   Randomized Complete Block Design with 3 repetitions (design structure).

We can easily come up with two models:

1.  Blocks fixed $$y_{ijk} = \mu + \tau_i + \rho_j + \varepsilon_{ijk}; \ \ \varepsilon \sim N(0, \sigma^2)$$.  
2.  Blocks random $$y_{ijk} = \mu + \tau_i + u_j + \varepsilon_{ijk}; \ \ u_j \sim N(0, \sigma^2_u) \varepsilon \sim N(0, \sigma^2) \ \text{and} \ \text{cov}(u, \varepsilon)=0$$.

{% capture text %}
**Notes on Notation**

-   scalars: lowercase italic and non-bold faced, e.g., $$y$$, $$\sigma$$, $$\beta_0$$  
-   vectors: lowercase bold, e.g., $$\mathbf{y} \equiv [y_1, y_2, ..., y_n]'$, $\boldsymbol{\beta} \equiv [\beta_1, \beta_2, ..., \beta_p]'$$, $$\boldsymbol{u}  \equiv [u_1, u_2, ..., u_k]'$$ (note that their elements may be scalars)  
-   matrices: uppercase bold, e.g., $$\mathbf{X}$$, $$\Sigma$$ (note that their elements may be vectors)  

| Variable | Scalar | Vector | Matrix |
|------------------|------------------|------------------|------------------|
| Response variable | $$y$$ (e.g., $$y = 4$$) | $$\mathbf{y} \equiv (y_1, y_2, ..., y_n)'$$ | $$\mathbf{y}_{n\times1}$$ |
| Predictor variable | $$x_{1 i}$$, $$x_{2 i}$$, etc. | $$\mathbf{x}_1 \equiv (x_{1,1}, x_{1, 2}, ..., x_{1, n})$$ $$\mathbf{x}_2 \equiv (x_{2,1}, x_{2, 2}, ..., x_{2, n})$$ | $$\mathbf{X}_{n\times p} \equiv \begin{bmatrix} \end{bmatrix}$$ |
| Effect parameters | $$\beta_0$$, $$\beta_1$$, etc. | $$\boldsymbol{\beta} \equiv (\beta_0, \beta_1, ..., \beta_p)'$$ | $$\boldsymbol{\beta}_{p\times1}$$ |
| Variance | $$\sigma^2$$ |  | $$\Sigma$$ (very often we assume $$\Sigma = \sigma^2 \mathbf{I}$$ ) |
|  |  |  |  |{% endcapture %}
{% include alert.html text=text color=secondary %}  


## What to expect next  

- More applied examples and what they mean.  
- Some troubleshooting.  

Any questions? E-mail me!
