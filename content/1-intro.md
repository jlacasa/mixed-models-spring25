---
title: Fundamentals of linear mixed models 
nav: Day 1
topics: Review; Fixed effects versus random effects
---

## Welcome!

- [About me](https://jlacasa.github.io/).
- About you.
- Frequent responses.   

{% include figure.html img="day1/attendees.jpg" alt="Attendees counts" caption="Distribution of Departments attending this worksop" width="75%" %}

## Housekeeping  

- We will have relatively low proportion of R code in this workshop. Instead, we will focus on the understanding of the model components. Questions/concerns are more than welcome.   

Schedule:

{% capture text %}
1. Fundamentals of linear mixed models
   Review of linear models & basic concepts of linear mixed models     
3. Modeling data generated by designed experiments
5. Generalized linear mixed models (aka non-normal response) applied to designed experiments  
{% endcapture %}
{% include card.html text=text header="Workshop Overview" %}


## Outline for today

-   **Review on linear models**: Refresh (all-fixed-effects) linear models, and get to mixed-effects models by adding an extra assumption. 
-   **Review on variance-covariance matrices**: this is important because the information of the random effects is stored in the variance-covariance matrix.  
-   **Fixed effects versus random effects**: Where does the information in the fixed effects and random effects go?
-   **Application**: what does this all mean when we want to fit a model to our data?

------

## Review on linear models

### The famous intercept-and-slope linear model

One of the most popular models is the intercept-and-slope model. Why? Because it's so simple and interpretable! Most of us learned this way of writing out the statistical model:

$$y_{i} = \beta_0 + x_{i} \beta_1 + \varepsilon_{i}, \\ \varepsilon_i \sim N(0, \sigma^2),$$ where $$y_{i}$$ is the observed value for the $$i$$th observation, $$\beta_0$$ is the intercept, $$\beta_1$$ is the slope parameter, and $$\varepsilon_{i}$$ is the difference between the observed ($$y$$) and the expected ($$E(y_i)=\mu_i=\beta_0+x_i\beta_1$$) and that's why we often call it "residual".

[[plot]]

Statistics is all about making assumptions to get value out of our data, so let's review the assumptions we make in this model.  
- Linearity  
- Constant variance  
- Independence  
- Normality  

There is another way of writing out the statistical model above:  

$$y_{i} \sim N(\mu_i, sigma^2),$$  
$$\mu_i = \beta_0 + x_{i} \beta_1.$$

{% include figure.html img="day1/normal_univariate.png" alt="Univariate Normal distributions" caption="Normal distributions" width="75%" %}

**Discuss in the plot above:**    
-   Expected value  
-   Variance  
-   Covariance?

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right)$$


### Review of the statistical model using matrix notation


Let's identify each in the following example:

$$\mathbf{y}_{n \times 1} \sim N(\boldsymbol{\mu}_{n \times 1}, \sigma^2\mathbf{I}_{n \times n}),  $$ where $$\mathbf{I}_{n \times n}$$ is the identity matrix with $$n$$ rows and $$n$$ columns. Suppose $$n = 4$$, then $$\mathbf{I}_{4 \times 4} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1  \end{bmatrix}$$.


### Review on variance-covariance matrices  

$$
\begin{array}{c c} 
& \begin{array}{c c c} a & b &c \\ \end{array} \\
\begin{array}{c c c}p\\q\\r \end{array} &
\left[
\begin{array}{c c c}
.1 & .1 & 0 \\
.4 & 1 & 0 \\
.8 & 0 & .4
\end{array}
\right]
\end{array}
$$

- Short demonstration of different variance-covariance functions using R. [[see R code](#)]  


## What are mixed models anyways?

Mixed models (also called "multilevel models") model some of their parameters (i.e., regression coefficients) with probability distributions.

### Fixed effects versus random effects  


#### Method of estimation  

- REML is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased.
- Why is the unbiased estimation of variance components so important?  
  - Relationship between variance estimates, standard error, confidence intervals, t-tests, type I error.


## Applied example  
**Example:** Randomized complete block design.  

-   Field experiment at Colby, KS.  
-   One treatment factor (treatment structure).  
-   Randomized Complete Block Design with 3 repetitions (design structure).

We can easily come up with two models:

1.  Blocks fixed $$y_{ijk} = \mu + \tau_i + \rho_j + \varepsilon_{ijk}; \ \ \varepsilon \sim N(0, \sigma^2)$$.  
2.  Blocks random $$y_{ijk} = \mu + \tau_i + u_j + \varepsilon_{ijk}; \ \ u_j \sim N(0, \sigma^2_u) \varepsilon \sim N(0, \sigma^2) \ \text{and} \ \text{cov}(u, \varepsilon)=0$$.

{% capture text %}
**Notes on Notation**

-   scalars: lowercase italic and non-bold faced, e.g., $$y$$, $$\sigma$$, $$\beta_0$$  
-   vectors: lowercase bold, e.g., $$\mathbf{y} \equiv [y_1, y_2, ..., y_n]'$, $\boldsymbol{\beta} \equiv [\beta_1, \beta_2, ..., \beta_p]'$$, $$\boldsymbol{u}  \equiv [u_1, u_2, ..., u_k]'$$ (note that their elements may be scalars)  
-   matrices: uppercase bold, e.g., $$\mathbf{X}$$, $$\Sigma$$ (note that their elements may be vectors)  

| Variable | Scalar | Vector | Matrix |
|------------------|------------------|------------------|------------------|
| Response variable | $$y$$ (e.g., $$y = 4$$) | $$\mathbf{y} \equiv (y_1, y_2, ..., y_n)'$$ | $$\mathbf{y}_{n\times1}$$ |
| Predictor variable | $$x_{1 i}$$, $$x_{2 i}$$, etc. | $$\mathbf{x}_1 \equiv (x_{1,1}, x_{1, 2}, ..., x_{1, n})$$ $$\mathbf{x}_2 \equiv (x_{2,1}, x_{2, 2}, ..., x_{2, n})$$ | $$\mathbf{X}_{n\times p} \equiv \begin{bmatrix} \end{bmatrix}$$ |
| Effect parameters | $$\beta_0$$, $$\beta_1$$, etc. | $$\boldsymbol{\beta} \equiv (\beta_0, \beta_1, ..., \beta_p)'$$ | $$\boldsymbol{\beta}_{p\times1}$$ |
| Variance | $$\sigma^2$$ |  | $$\Sigma$$ (very often we assume $$\Sigma = \sigma^2 \mathbf{I}$$ ) |
|  |  |  |  |{% endcapture %}
{% include alert.html text=text color=secondary %}  


## What to expect next  

- More applied examples and what they mean.  
- Some troubleshooting.  

Any questions? E-mail me!
