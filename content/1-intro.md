---
title: Fundamentals of linear mixed models 
nav: Day 1
topics: Review; Fixed effects versus random effects
---

## Welcome!

- [About me](https://jlacasa.github.io/).
- About you.
- Frequent responses.   
- Some knowledge of the *existence* of mixed effects models.  

{% include figure.html img="day1/attendees.jpg" alt="Attendees counts" caption="Figure 1. Distribution of Departments attending this worksop" width="75%" #attendees %}

## Housekeeping  

- We will have relatively low proportion of R code in this workshop. Instead, we will focus on the understanding of the model components. Questions/concerns are more than welcome.   

Schedule:

{% capture text %}
1. Fundamentals of linear mixed models
   Review of linear models & basic concepts of linear mixed models     
3. Modeling data generated by designed experiments
5. Generalized linear mixed models (aka non-normal response) applied to designed experiments  
{% endcapture %}
{% include card.html text=text header="Workshop Overview" %}


## Outline for today

-   **Review on linear models**: Refresh (all-fixed-effects) linear models, and get to mixed-effects models by adding an extra assumption. 
-   **Review on variance-covariance matrices**: this is important because the information of the random effects is stored in the variance-covariance matrix.  
-   **Fixed effects versus random effects**: Where does the information in the fixed effects and random effects go?
-   **Application**: what does this all mean when we want to fit a model to our data?

------

## Review on linear models

### The famous intercept-and-slope linear model

{% include figure.html img="day1/linear_regression_1.jpg" alt="" caption="Figure 2. A good example for the intercept-and-slope model: Apple diameter versus time." width="75%" #intercept_slope_fig1 %}

One of the most popular models is the intercept-and-slope model. Why? Because it's so simple and interpretable! Most of us learned this way of writing out the statistical model:

$$y_{i} = \beta_0 + x_{i} \beta_1 + \varepsilon_{i}, \\ \varepsilon_i \sim N(0, \sigma^2),$$  

where $$y_{i}$$ is the observed value for the $$i$$th observation, $$\beta_0$$ is the intercept, $$\beta_1$$ is the slope parameter, and $$\varepsilon_{i}$$ is the difference between the observed ($$y$$) and the expected ($$E(y_i)=\mu_i=\beta_0+x_i\beta_1$$) and that's why we often call it "residual".


Look at the plot above. A farmer decided to measure the diameter or **random apples from random trees** at different points in time.  

Can we fit the model to that data? Let's review the assumptions we make in this model.   
- Linearity  
- Constant variance  
- Independence  
- Normality  


### Same statistical model using distribution notation and matrix notation

There is another way of writing out the intercept-and-slope statistical model above:  

$$y_{i} \sim N(\mu_i, \sigma^2),$$  
$$\mu_i = \beta_0 + x_{i} \beta_1.$$

Also,  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),$$  
$$\boldsymbol{\mu} = \boldsymbol{\beta_0} + x \boldsymbol{\beta_1}.$$

The advantages of writing out statistical models with this type of notation are 

i. It is easier to switch to other distributions (Day 3 of this workshop)  
ii. It is easier to define and understand the variance-covariance, especially in a mixed models scenario!  

For now, let's focus on the normal distribution...  

### Review on variance-covariance matrices  

#### What is variance?  

Random variables are usually described with their properties like the expected value and variance. 
The expected value and variance are the first and second central moments of a distribution, respectively. 
Regardless of the distribution of a random variable $$Y$$, we could calculate its expected value $$E(Y)$$ and variance $$Var(Y)$$.  
The expected value measures the average outcome of $$Y$$.  
The variance measures the dispersion of $$Y$$, i.e. how far the possible outcomes are spread out from their average. 

{% include figure.html img="day1/normal_univariate.png" alt="Univariate Normal distributions" caption="Figure 3. Normal distributions" width="75%" #univariate_normal %}

**Discuss in the plot above:**    
-   Expected value  
-   Variance  
-   Covariance?

#### What is covariance?  

When two random variables behave similarly. For example, let's take two variables $$y_1$$ and $$y_2$$ that have variances of 1 each, and also a covariance of 0.6 [Figure 4](#multivariate_normal). We can write that out as 
The variance of a random variable is the covariance of a random variable with itself.  

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right).$$

{% include figure.html img="day1/normal_multivariate.jpg" alt="Multivariate Normal distribution" caption="Figure 4. Multivariate Normal distribution showing the correlation between two random normal variables.$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right).$$" width="75%" #multivariate_normal %}

Back to the example in [Figure 1](#attendees). Let's assume we have 10 observations of diameter of random apples. Then,   

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma)$$  

$$\begin{array}{c c}
& \mathbf{y} \equiv \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ \vdots \\ y_n \end{bmatrix} & 
\Sigma \equiv \sigma^2 
\begin{bmatrix} 1 & 0 & 0 & 0 & \dots & 0 \\ 
0 & 1 & 0 & 0 & \dots & 0 \\
0 & 0 & 1 & 0 & \dots & 0 \\
0 & 0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\  
0 & 0 & 0 & 0 & \dots & 1 \end{bmatrix}
\\
\end{array}
$$


### Adding a random effect to the model   

Now, imagine that the observations are actually diameters from random apples from 5 different fields. 
We expect the growth rate to be similar, but the baseline (a.k.a., the intercept) to be field-specific. Then,   

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  

#### How do we define $$\beta_{0j}$$?

So far, we could have defined an all-fixed model. 

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \beta_{0j} = \beta_0 + b_j \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  

That yields us 

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma)$$  

$$\Sigma = \begin{bmatrix} \sigma^2 + \sigma^2_u & \sigma^2_u & 0 & 0 & 0 & 0 &\dots & 0\\
\sigma^2_u & \sigma^2 + \sigma^2_u & 0 & 0 & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & \dots & \vdots \\


\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & 0 & 0 & \dots & \sigma^2 + \sigma^2_u
\end{bmatrix} $$


### Covariance functions 

[[see R code](#)]  


## What are mixed models anyways?

Mixed models combine fixed effects and random effects. 

### Random effects  

- By definition, random effects are parameters that. 
- Typically, a random effect $$u \sim N(0, \sigma^2_u)$$.   
- In the context of designed experiments, random effects are assumed to be independent to each other and independent to the residual.  
- Method of estimation  
  - REML is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased.
  - *Why is the unbiased estimation of variance components so important?*  
    - Relationship between variance estimates, standard error, confidence intervals, t-tests, type I error.



### Fixed effects versus random effects  

**Group discussion:** 
- What determines if an effect should be random of fixed?  




## Applied example  
**Example:** Randomized complete block design.  

-   Field experiment at Colby, KS.  
-   One treatment factor (treatment structure).  
-   Randomized Complete Block Design with 3 repetitions (design structure).  

We can easily come up with two models:

1.  Blocks fixed $$y_{ijk} = \mu + \tau_i + \rho_j + \varepsilon_{ijk}; \ \ \varepsilon \sim N(0, \sigma^2)$$.  
2.  Blocks random $$y_{ijk} = \mu + \tau_i + u_j + \varepsilon_{ijk}; \ \ u_j \sim N(0, \sigma^2_u) \varepsilon \sim N(0, \sigma^2) \ \text{and} \ \text{cov}(u, \varepsilon)=0$$.


## What to expect next  

- Friday, same time, same place.  
- More applied examples and what they mean.  
- Some troubleshooting.  

Any questions? E-mail me!  


