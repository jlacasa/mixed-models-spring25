---
title: Fundamentals of linear mixed models 
nav: Day 1
topics: Review; Fixed effects versus random effects
---

- [Welcome!](#welcome-)
- [Housekeeping](#housekeeping)
- [Outline for today](#outline-for-today)
- [Review on linear models](#review-on-linear-models)
  * [The famous intercept-and-slope linear model](#the-famous-intercept-and-slope-linear-model)
  * [Let's fit the same statistical model using distribution notation and matrix notation](#let-s-fit-the-same-statistical-model-using-distribution-notation-and-matrix-notation)
  * [Review on variance-covariance matrices](#review-on-variance-covariance-matrices)
    + [What does "the variance of $$y$$" even mean?](#what-does--the-variance-of---y----even-mean-)
    + [On the covariance of two random variables $$y_1$$ and $$y_2$$](#on-the-covariance-of-two-random-variables---y-1---and---y-2--)
  * [Adding a random effect to the model](#adding-a-random-effect-to-the-model)
    + [Independent observations](#independent-observations)
    + [Non-independent observations](#non-independent-observations)
    + [How do we define $$\beta_{0j}$$?](#how-do-we-define----beta--0j----)
      - [Fixed](#fixed)
      - [Random](#random)
- [What are mixed models anyways?](#what-are-mixed-models-anyways-)
  * [Random effects](#random-effects)
  * [Speaking generally](#speaking-generally)
  * [Fixed effects versus random effects](#fixed-effects-versus-random-effects)
- [Applied example](#applied-example)
- [Wrap-up](#wrap-up)
- [What's next](#what-s-next)"

------------------

## Welcome!

- [About me](https://jlacasa.github.io/).
- About you.
  - Frequent responses to the registration survey.   
  - Some knowledge of the *existence* of mixed effects models.  
  - Mostly life sciences - often heteroscedasticity and dependent observations!  

{% include figure.html img="day1/attendees.jpg" alt="Attendees counts" caption="Figure 1. Distribution of Departments attending this worksop" width="75%" id = "attendees" %}

## Housekeeping  

- We will have relatively low proportion of R code in this workshop. Instead, we will focus on the understanding of the model components. Questions/concerns are more than welcome.   
- Emphasis on modeling and figuring out what mixed models actually do.  
- The statistical notation we will use throughout this workshop is presented [here](0-prep). 
- DON'T PANIC when you see the math notation. 
It is **not expected** that you walk out of this workshop as a master in math notation! But the variance-covariance functions are important to understand what mixed models actually do.  
- Schedule:

{% capture text %}
1. Fundamentals of linear mixed models  
   Review of linear models & basic concepts of linear mixed models     
3. Modeling data generated by designed experiments
5. Generalized linear mixed models (aka non-normal response) applied to designed experiments  
{% endcapture %}
{% include card.html text=text header="Workshop Overview" %}


## Outline for today

-   **Review on linear models**: Refresh (all-fixed-effects) linear models, and get to mixed-effects models by adding an extra assumption. 
-   **Review on variance-covariance matrices**: this is important because the information of the random effects is stored in the variance-covariance matrix.  
-   **Fixed effects versus random effects**: Where does the information in the fixed effects and random effects go?
-   **Application**: what does this all mean when we want to fit a model to our data?

------

## Review on linear models

### The famous intercept-and-slope linear model

{% include figure.html img="day1/linear_regression_1.jpg" alt="" caption="Figure 2. A good example for the intercept-and-slope model: Apple diameter versus time." width="75%" id = "intercept_slope_fig1" %}

One of the most popular models is the intercept-and-slope model. 
It's so simple and [interpretable](https://en.wikipedia.org/wiki/Simple_linear_regression#Interpretation)! 
Most of us learned this way of writing out the statistical model, called "model equation form":

$$y_{i} = \beta_0 + x_{i} \beta_1 + \varepsilon_{i}, \\ \varepsilon_i \sim N(0, \sigma^2),$$  

where $$y_{i}$$ is the observed value for the $$i$$th observation, 
$$\beta_0$$ is the intercept (i.e., the expected value of $$y$$ when $$x=0$$), 
$$\beta_1$$ is the slope parameter (i.e., the expected increase in $$y$$ with a unity increase in $$x$$), 
$$x_i$$ is the predictor for the $$i$$th observation, 
and $$\varepsilon_{i}$$ is the difference between the observed value $$y$$ 
and the expected value $$E(y_i)=\mu_i=\beta_0+x_i\beta_1$$ - that's why we often call it "residual". 
Typically, $$\boldsymbol{\beta} \equiv \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}$$ 
is estimated via maximum likelihood estimation. Also, when we assume a Normal distribution, 
maximum likelihood estimation yields the same point estimates as least squares estimation. 


Look at the plot above. A farmer decided to measure the diameter of apples **selected randomly from randomly selected trees** at different points in time.  

Can we fit the model to that data? Let's review the assumptions we make in this model (which is the default model in most software).   
- Linearity  
- Constant variance  
- Independence  
- Normality  


### Let's fit the same statistical model using distribution notation and matrix notation

There are other ways of writing out the intercept-and-slope statistical model above. 

Instead of focusing on the distribution of the residuals, we can focus on the distribution of the data $$y$$:  

$$y_{i} \sim N(\mu_i, \sigma^2), \\ \mu_i = \beta_0 + x_{i} \beta_1.$$

With this notation (called "probability distribution form"), it is easier to switch to other distributions ([Day 3](3-lesson) of this workshop). We can further express this equation using vectors and matrices:  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma), \\ \boldsymbol{\mu} = \boldsymbol{1} \boldsymbol{\beta_0} + \mathbf{x} \boldsymbol{\beta_1} = \mathbf{X}\boldsymbol{\beta},$$

where $$n$$ is the total number of observations, $$p$$ is the total number of parameters, 
$$\mathbf{y}$$ is an $$n \times 1$$ vector containing all observations, 
$$\boldsymbol{\mu}$$ is an $$n \times 1$$ vector containing the expected values of said observations, 
$$\boldsymbol{\beta}$$ is an $$p \times 1$$ vector containing the (fixed) parameters, 
$$\mathbf{X}$$ is an $$n \times p$$ matrix containing the predictors, 
$$\Sigma$$ is an $$n \times n$$ matrix called variance-covariance matrix. 
*Note: This type of notation is also convenient to think about how to prepare the data in your spreadsheet. One row per observation (rows in $$\mathbf{X}$$), one variable per column (columns in $$\mathbf{X}$$).*

Let's expand these expressions: 

$$\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \sim  N \left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} Cov(y_1, y_1) & Cov(y_1, y_2) & \dots & Cov(y_1, y_n) \\
Cov(y_2, y_1) & Cov(y_2, y_2) & \dots & Cov(y_2, y_n)\\
\vdots & \vdots & \ddots & \vdots \\ 
Cov(y_n, y_1) & Cov(y_n, y_2) & \dots & Cov(y_n, y_n) \end{bmatrix} \right),$$

which is the same as 

$$\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \sim N \left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} Var(y_1) & Cov(y_1, y_2) & \dots & Cov(y_1, y_n) \\
Cov(y_2, y_1) & Var(y_2) & \dots & Cov(y_2, y_n)\\
\vdots & \vdots & \ddots & \vdots \\ 
Cov(y_n, y_1) & Cov(y_n, y_2) & \dots & Var(y_n) \end{bmatrix} \right).$$

It is easier to define and understand the variance-covariance when we look directly at vectors and matrices. This will come particularly handy in our mixed models applications! 


### Review on variance-covariance matrices  

#### What does "the variance of $$y$$" even mean?  

Random variables are usually described with their properties like the expected value and variance. 
The expected value and variance are the first and second central moments of a distribution, respectively. 
Regardless of the distribution of a random variable $$Y$$, we could calculate its expected value $$E(Y)$$ and variance $$Var(Y)$$. 
The expected value measures the average outcome of $$Y$$. 
The variance measures the dispersion of $$Y$$, i.e. how far the possible outcomes are spread out from their average. 

{% include figure.html img="day1/normal_univariate.png" alt="Univariate Normal distributions" caption="Figure 3. Normal distributions" width="75%" id = "univariate_normal" %}

**Discuss in the plot above:**  
-   Expected value  
-   Variance  
-   Covariance?

#### On the covariance of two random variables $$y_1$$ and $$y_2$$    

Covariance between two random variables means how the two random variables behave relative to each other. The variance of a random variable is the covariance of a random variable with itself. Let's take two variables $$y_1$$ and $$y_2$$ that have variances of 1 each, and also a covariance of 0.6 [Figure 4](#multivariate_normal). We can write that out as 

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right),$$

which is visualized in [Figure 4](#multivariate_normal).   

{% include figure.html img="day1/normal_multivariate.jpg" alt="Multivariate Normal distribution" caption="Figure 4. Multivariate Normal distribution showing the correlation between two random normal variables." width="75%" id = "multivariate_normal" %}

**Discuss in the plot above:**  
-   Expected value  
-   Variance  
-   Covariance 

## Adding a random effect to the model   

### Independent observations  

Back to the example in [Figure 2](#intercept_slope_fig1). Let's assume we have $$n$$ observations of diameter of apples. 
The apples were randomly selected from random trees from a field. 

{% include modal.html button="Example data" color="success" title="Example data" 
text='<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fixed vs Random Effects Table</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
    </style>
</head>

<body>
    <table>
        <tr>
            <th>day</th>
            <th>diameter_cm</th>
            <th>field</th>
        </tr>
        <tr>
            <td>3</td>
            <td>2.9</td>
            <td>A</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.8</td>
            <td>B</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.9</td>
            <td>C</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.7</td>
            <td>D</td>
        </tr>
        <tr>
            <td>3</td>
            <td>3.0</td>
            <td>E</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.6</td>
            <td>A</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.9</td>
            <td>B</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.7</td>
            <td>C</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.8</td>
            <td>D</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.7</td>
            <td>E</td>
        </tr>
    </table>
</body>' %}

If we used the default model in most software, we would assume  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),\\
\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ \vdots \\ y_n \end{bmatrix} \sim N
\left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \\ \vdots \\ \mu_n \end{bmatrix}, 
\sigma^2 
\begin{bmatrix} 1 & 0 & 0 & 0 & \dots & 0 \\ 
0 & 1 & 0 & 0 & \dots & 0 \\
0 & 0 & 1 & 0 & \dots & 0 \\
0 & 0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\  
0 & 0 & 0 & 0 & \dots & 1 \end{bmatrix}
\right),$$

which is the same as 

$$\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ \vdots \\ y_n \end{bmatrix} \sim N
\left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} \sigma^2 & 0 & 0 & 0 & \dots & 0 \\ 
0 & \sigma^2 & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 & 0 & \dots & 0 \\
0 & 0 & 0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\  
0 & 0 & 0 & 0 & \dots & \sigma^2 \end{bmatrix}
\right).$$

Remember the assumptions:
- Linearity  
- Constant variance  
- Independence  
- Normality  

These assumptions makes sense when the observations are independent (i.e., there is no underlying structure to take into account).    

### Non-independent observations  

Now, imagine that the observations are actually diameters from random apples, but they were taken from 5 different fields (2 from each field). 
These observations are no longer independent, because apples from the same field are more similar to each other than apples from different fields.
This is when mixed-effects models enter the story - they allow us to indicate *what is similar to what* via random effects. 
In this case, we expect the growth rate to be similar among fields, but the baseline (a.k.a., the intercept) to be field-specific. Then,   

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  

### How do we define $$\beta_{0j}$$?

#### Fixed   

So far, we could have defined an all-fixed model. 

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \beta_{0j} = \beta_0 + u_j \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  

where $$u_j$$ is the effect of the $$j$$th field on the intercept (i.e., on the baseline). 
In this case, $$u_j$$ is a fixed effect, which means it may be estimated via least squares estimation or maximum likelihood estimation. 
Under both least squares and maximum likelihood (assuming normal distribution), we may estimate the parameters by computing 

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^TX)^{-1}\mathbf{X}^T\mathbf{y},$$

which yields the minimum variance unbiased estimate of $$\boldsymbol{\beta}$$. 

#### Random   

We could also assume that the effects of the $$j$$th tree (i.e., $$b_j$$) arise from a random distribution. 
The most common assumption (and the default in most statistical software) is that 

$$u_j \sim N(0, \sigma^2_b).$$

Now, we don't estimate the effect, but the variance $$\sigma^2_b$$.  

## Generalities -- what are mixed models anyways?

Mixed models combine fixed effects and random effects. 


Generally speaking, we can write out a mixed-effects model using the model equation form, as   

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{u} + \boldsymbol{\varepsilon}, \\ 
\begin{bmatrix}\mathbf{u} \\ \boldsymbol{\varepsilon} \end{bmatrix} \sim \left(
\begin{bmatrix}\boldsymbol{0} \\ \boldsymbol{0} \end{bmatrix}, 
\begin{bmatrix}\mathbf{G} & \boldsymbol{0} \\
\boldsymbol{0} & \mathbf{R} \end{bmatrix} 
\right),$$

where $$\mathbf{y}$$ is the observed response, 
$$\mathbf{X}$$ is the matrix with the explanatory variables, 
$$\mathbf{Z}$$ is the design matrix,
$$\boldsymbol{\beta}$$ is the vector containing the fixed-effects parameters, 
$$\mathbf{u}$$ is the vector containing the random effects parameters, 
$$\boldsymbol{\varepsilon}$$ is the vector containing the residuals, 
$$\mathbf{G}$$ is the variance-covariance matrix of the random effects, 
and $$\mathbf{R}$$ is the variance-covariance matrix of the residuals. 

{% include modal.html button="Example for <strong>X</strong> and <strong>Z</strong>" color="success" 
title="Example for <strong>X</strong> and <strong>Z</strong>" 
text="<strong>Example A.</strong> Let's focus on the first 10 observations of apple diameter. 
Said first 10 observations of apple diameters include days 3 and 6 (which you can find in <strong>X</strong>), and one observation per field for each day (which you can find in <strong>Z</strong>). $$\mathbf{X} = \begin{array}{cc}  
\text{Int} \phantom{-} \text{day} \\ 
\begin{bmatrix} 
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 6 \\
1 & 6 \\
1 & 6 \\
1 & 6 \\
1 & 6  
\end{bmatrix} 
\end{array}$$, $$\mathbf{Z} = \begin{array}{cc}
\text{f}1 \phantom{-} \text{f}2 \phantom{-} \text{f}3 \phantom{-} \text{f}4 \phantom{-} \text{f}5 \\ 
\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \end{bmatrix}
\end{array}$$

------

<strong>Example B.</strong> Let's focus on the first 10 observations of apple diameter. 
In this case, we aim to predict <strong>final</strong> diameter based on the apple variety: Red delicious (RD), Gala (G) or Fuji(F).
You can still find this information in <strong>X</strong>, . The <strong>Z</strong> matrix remains the same. 

$$\begin{array}{ccc}  
& \text{RD} \phantom{-} \text{G} \phantom{-} \text{F} \\ 
\mathbf{X} = &
\begin{bmatrix} 
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 
\end{bmatrix} 
\end{array}$$, $$\mathbf{Z} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \end{bmatrix}$$
" %}

Using the probability distribution form, we can then say that $$E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$$ and $$Var(\mathbf{y}) = \mathbf{Z}\mathbf{G}\mathbf{Z}' + \mathbf{R}$$. Then,   

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),$$  

$$\Sigma = \begin{bmatrix} \sigma^2 + \sigma^2_u & \sigma^2_u & 0 & 0 & 0 & 0 &\dots & 0\\
\sigma^2_u & \sigma^2 + \sigma^2_u & 0 & 0 & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & \dots & \vdots \\


\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & 0 & 0 & 0 & \dots & \sigma^2 + \sigma^2_u
\end{bmatrix}.$$

Take your time to digest the variance-covariance matrix above. What type of data do you think generated it? 

### Random effects    

- By definition, random effects are regression coefficients that arise from a random distribution. 
- Typically, a random effect $$u \sim N(0, \sigma^2_u)$$.   
- We estimate the variance $$\sigma^2_u$$.  
- In the context of designed experiments, random effects are assumed to be independent to each other and independent to the residual.  


Restricted maximum likelihood estimation (REML) is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased.
- In REML, the likelihood is maximized after accounting for the model’s fixed effects.  

- In ML, $$-\ell_{ML}(\boldsymbol{\sigma; \boldsymbol{\beta}, \mathbf{y}}) = - (\frac{n}{2}) \log(2\pi)-(\frac{1}{2}) \log ( \vert \mathbf{V}(\boldsymbol\sigma) \vert ) - (\frac{1}{2}) (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^T[\mathbf{V}(\boldsymbol\sigma)]^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})$$  
- In REML, $$\ell_{REML}(\boldsymbol{\sigma};\mathbf{y}) = - (\frac{n-p}{2}) \log (2\pi) - (\frac{1}{2}) \log ( \vert \mathbf{V}(\boldsymbol\sigma) \vert ) - (\frac{1}{2})log \left(  \vert \mathbf{X}^T[\mathbf{V}(\boldsymbol\sigma)]^{-1}\mathbf{X} \vert \right) - (\frac{1}{2})\mathbf{r}[\mathbf{V}(\boldsymbol\sigma)]^{-1}\mathbf{r}$$, where $$p = rank(\mathbf{X})$$ and $$\mathbf{r} = \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}_{ML}$$.  
  - Start with initial values for $$\boldsymbol{\sigma}$$, $$\tilde{\boldsymbol{\sigma}}$$.  
  - Compute $$\mathbf{G}(\tilde{\boldsymbol{\sigma}})$$ and $$\mathbf{R}(\tilde{\boldsymbol{\sigma}})$$.  
  - Obtain $$\boldsymbol{\beta}$$ and $$\mathbf{b}$$.   
  - Update $$\tilde{\boldsymbol{\sigma}}$$.  
  - Repeat until convergence.  


### Fixed effects versus random effects  

**Group discussion:** what determines if an effect should be random of fixed? 
Consider the assumptions:  

- $$\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \frac{\sigma^2}{(n-1)s^2_\mathbf{x}})$$  
- $$u_j \sim N(0, \sigma^2_u)$$  
- What process is being studied?  
- How were the levels selected? (randomly, carefully selected)  
- How many levels does the factor have, vs. how many did we observe?   

Some good references:  
- Page 20 in Gelman (2005). "Analysis of variance—why it is more important than ever". [[link](https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-1/Analysis-of-variancewhy-it-is-more-important-than-ever/10.1214/009053604000001048.full)]


## Applied example  

[[R code](#)]

-   Field experiment at Colby, KS.  
-   One treatment factor: genotype (treatment structure).  
-   Randomized Complete Block Design with 3 repetitions (design structure).  

{% include figure.html img="day1/applied_example_rcbd.jpg" alt="" caption="Figure 5. Designed experiment. Colors indicate different genotypes." width="50%" id = "applied_ex" %}


We can easily come up with two models:

1.  Blocks fixed $$y_{ijk} = \mu + \tau_i + \rho_j + \varepsilon_{ijk}; \ \ \varepsilon \sim N(0, \sigma^2)$$.  
2.  Blocks random $$y_{ijk} = \mu + \tau_i + u_j + \varepsilon_{ijk}; \ \ u_j \sim N(0, \sigma^2_u) \varepsilon \sim N(0, \sigma^2) \ \text{and} \ \text{cov}(u, \varepsilon)=0$$.

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embed R Code</title>
    <style>
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow-x: auto; /* Enables horizontal scrolling if the code is too wide */
        }
    </style>
</head>
<body>
    <pre>
<code>
library(tidyverse)
library(glmmTMB)

data(gilmour.serpentine)
dd <- gilmour.serpentine

m1 <- glmmTMB(yield ~ 1 + gen + rep, data = dd)
m2 <- glmmTMB(yield ~ 1 + gen + (1|rep), data = dd)
</code>
    </pre>
</body>
</html>

------

## Wrap-up  

- Assumptions in mixed models. 
- 

## What's next  

- Friday, same time, same place.  
- More applied examples and what they mean.  
- Some troubleshooting.  

Any questions? E-mail me!  


