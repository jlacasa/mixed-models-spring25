---
title: Modeling data generated by designed experiments
nav: Day 2
topics: Designed Experiments; Randomized Complete Block Designs; Split-Plot-Designs; Repeated Measures
---

{% capture text %}
Mixed-effects models combine fixed effects and random effects. 
Typically, we can define a mixed-effects model as 

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{u} + \boldsymbol{\varepsilon}, \\ 
\begin{bmatrix}\mathbf{u} \\ \boldsymbol{\varepsilon} \end{bmatrix} \sim \left(
\begin{bmatrix}\boldsymbol{0} \\ \boldsymbol{0} \end{bmatrix}, 
\begin{bmatrix}\mathbf{G} & \boldsymbol{0} \\
\boldsymbol{0} & \mathbf{R} \end{bmatrix} 
\right),$$

where $$\mathbf{y}$$ is the observed response, 
$$\mathbf{X}$$ is the matrix with the explanatory variables, 
$$\mathbf{Z}$$ is the design matrix,
$$\boldsymbol{\beta}$$ is the vector containing the fixed-effects parameters, 
$$\mathbf{u}$$ is the vector containing the random effects parameters, 
$$\boldsymbol{\varepsilon}$$ is the vector containing the residuals, 
$$\mathbf{G}$$ is the variance-covariance matrix of the random effects, 
and $$\mathbf{R}$$ is the variance-covariance matrix of the residuals. 
Typically, $$\mathbf{G} = \sigma^2_u \mathbf{I}$$ and $$\mathbf{R} = \sigma^2 \mathbf{I}$$.  
If we do the math, we get that  

$$E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta},$$

$$Var(\mathbf{y}) = \mathbf{Z}\mathbf{G}\mathbf{Z}' + \mathbf{R}$$

**Fixed effects versus random effects**  

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fixed vs Random Effects Table</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
    </style>
</head>

<body>

<table>
    <tr>
        <th> </th>
        <th>Fixed effects</th>
        <th>Random effects</th>
    </tr>
    <tr>
        <th>Where</th>
        <td>Expected value</td>
        <td>Variance-covariance matrix</td>
    </tr>
    <tr>
        <th>Inference</th>
        <td>Constant for all groups in the population of study</td>
        <td>Differ from group to group</td>
    </tr>
    <tr>
        <th>Usually used to model</th>
        <td>Carefully selected treatments or genotypes</td>
        <td>The study design (aka structure in the data, or what is similar to what)</td>
    </tr>
    <tr>
        <th>Assumptions</th>
        <td>$$\hat{\boldsymbol{\beta}} \sim N \left( \boldsymbol{\beta}, (\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X})^{-1} \right) $$</td>
        <td>$$u_j \sim N(0, \sigma^2_u)$$</td>
    </tr>
    <tr>
        <th>Method of estimation</th>
        <td>Maximum likelihood, least squares</td>
        <td>Restricted maximum likelihood (shrinkage)</td>
    </tr>
</table>
</body>

{% endcapture %}
{% include card.html text=text header= "Review" color="#a9d9a9" %}

## Outline for today

-   **Review on designed experiments.** 
-   **Should blocks be fixed or random?**  
-   **Applications of increased complexity**: random effects on the intercept, nested random effects, covariance functions (repeated measures).  

------

## Designed experiments  

**Golden Rules of designed experiments:**  
- Randomization  
- Replication  
- Local control  

**Experimental unit versus observational unit**  
- Experimental unit (EU): smallest unit to which a treatment is independently assigned.  
- Observational unit (OU): smallest unit on which observations are made. 
- If OU $$>$$ EU, then not all observations are independent. 
- The variance estimated from pseudoreplications (subsamples) is usually smaller than the variance estimated from true replications (experimental units). 

### Common types of designed experiments  

- Randomized complete block design (RCBD)  

<script type="text/tikz">
\begin{center}
    \tikz \node [scale=0.9, inner sep=0] {
                \begin{tikzpicture}[
                    region/.style={
                        draw=black!50,
                        dashed,
                    },
                    Node/.style={
                        midway,
                        red,
                    },
                    declare function={
                        xmin=0;
                        xmax=9;
                        ymin=0;
                        ymax=10;
                    }
                    ]
                    \begin{axis}[%hide axis,
                        xlabel=Longitude,
                        ylabel=Latitude,
                        xmin=xmin,
                        xmax=xmax,
                        ymin=ymin,
                        ymax=ymax,
                        axis background/.style={},
                        extra x ticks={},
                        extra y ticks={},
                        title=Block layout,
                        ]
                            \draw [region,fill=brown!45] (xmin,ymin) rectangle (3,ymax)  node [Node, color = black, xshift=0, yshift=0] {Block 1};
                            \draw [region,fill=brown!70] (3,ymin) rectangle (6,ymax)  node [Node, color = black, xshift=0, yshift=0] {Block 2};
                            \draw [region,fill=brown!30] (6,ymin)  rectangle (xmax,ymax)  node [Node, color = black, xshift=0, yshift=0] {Block 3};
                        \end{axis}
                \end{tikzpicture}
                }
    \end{center}</script>

- Split-plot design    

- Repeated measures

------

## Applied example I - random effect on the intercept   

The data below were generated by an experiment comparing sorghum genotypes [(Omer et al., 2015)](http://agrobiol.sggw.pl/~cbcs/articles/CBCS_10_2_4.pdf). 
The data presented here correspond to a randomized complete block design ( design structure) 
that was performed to study different genotypes. 
Remember that blocks are assumed to be aproximately homogeneous within. 
A reasonable model, then, would be  

$$y_{ij} = \mu + \tau_i + u_j + \varepsilon_{ij},\\
u_j \sim N(0, \sigma^2_u), \\
\varepsilon_{ij} \sim N(0, \sigma^2),$$

where $$y_{ij}$$ is the observed yield of the $$i$$th genotype in the $$j$$th block, 
$$\mu$$ is the overall mean, 
$$\tau_i$$ is the (fixed) effect of the $$i$$th genotype, 
$$u_j$$ is the (random) effect of the $$j$$th block, 
$$\varepsilon_{ij}$$ is the residual (i.e., the difference between predicted and observed) of the $$i$$th genotype in the $$j$$th block,
$$\sigma^2_u$$ is the variance among blocks, 
$$\sigma^2$$ is the residual variance. 

Since blocks are our only random effect, $$\mathbf{Z}$$ can be defined as 

$$\mathbf{Z} = \begin{array}{cc}
\text{b}1 \ \ \text{b}2 \ \text{b}3 \ \ \text{b}4 \\ 
\begin{bmatrix} 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 1\end{bmatrix}
\end{array}.$$

Note that the matrix columns are labeled according to the block they are indicating.   
If $$z_{ij} = 1$$, that means that that observation ($$i$$th row) belongs to that block ($$j$$th column).  
Otherwise, (i.e., $$z_{ij} = 0$$), that observation did not belong to that block. 

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title></title>
    <style>
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow-x: auto; /* Enables horizontal scrolling if the code is too wide */
        }
    </style>
</head>
<body>
    <pre>
<code>
library(glmmTMB)
library(agridat)
library(tidyverse)
library(DHARMa)

data(omer.sorghum)
dat <- omer.sorghum
dat <- dat %>% 
  filter(env == "E3")

m_rcbd <- glmmTMB(yield ~ gen + (1|rep), data = dat)

simulateResiduals(m_rcbd, plot = TRUE)
</code>
    </pre>
</body>
</html>

{% include figure.html img="day2/DHARMa_rcbd.png" alt="" caption="" width="80%" %}

{% highlight text %}
## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. 
##  
## Scaled residual values: 0.208 0.336 0.04 0.06 0.868 0.876 0.864 0.848 0.532 0.904 0.964 0.624 0.996 0.944 0.728 0.136 0.992 0.984 0.184 0.492 ...
{% endhighlight %}


{% highlight r %}
car::Anova(m_rcbd)
{% endhighlight %}



{% highlight text %}
## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: yield
##      Chisq Df Pr(>Chisq)    
## gen 128.93 17  < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
{% endhighlight %}


{% highlight r %}
marginal_means <- emmeans(m_rcbd, ~gen)

cld(marginal_means, 
    adjust = "sidak", 
    Letters = letters)
{% endhighlight %}



{% highlight text %}
##  gen emmean   SE df lower.CL upper.CL .group
##  G17    285 73.1 52     56.1      514  a    
##  G06    315 73.1 52     85.5      544  a    
##  G02    515 73.1 52    286.2      744  ab   
##  G12    539 73.1 52    309.9      768  ab   
##  G01    582 73.1 52    352.7      811  abc  
##  G14    583 73.1 52    353.8      812  abc  
##  G16    605 73.1 52    375.5      834  abc  
##  G05    643 73.1 52    413.8      872  abc  
##  G15    647 73.1 52    417.5      876  abc  
##  G11    728 73.1 52    499.0      957   bcd 
##  G10    739 73.1 52    509.4      968   bcd 
##  G08    741 73.1 52    512.3      971   bcd 
##  G04    749 73.1 52    520.1      978   bcd 
##  G03    805 73.1 52    575.8     1034   bcd 
##  G09    813 73.1 52    584.0     1042   bcd 
##  G13    825 73.1 52    595.9     1054   bcd 
##  G07    937 73.1 52    707.9     1166    cd 
##  G18   1030 73.1 52    801.0     1259     d 
## 
## Confidence level used: 0.95 
## Conf-level adjustment: sidak method for 18 estimates 
## P value adjustment: sidak method for 153 tests 
## significance level used: alpha = 0.05 
## NOTE: If two or more means share the same grouping symbol,
##       then we cannot show them to be different.
##       But we also did not show them to be the same.
{% endhighlight %}



## Applied example II -- nested random effects    

This data example was first reported in [Yates (1935)](https://doi.org/10.2307/2983638). 
The data were generated by a spli-plot experiment (design structure) that was performed to study
the yield of oats as affected by oat genotype and nitrogen fertilization (treatment structure). 

$$y_{ijk} = \mu + \tau_i + \alpha_j + u_k + v_{i \vert k} \varepsilon_{ijk},\\
u_k \sim N(0, \sigma^2_u), \\
v_{i \vert k} \sim N(0, \sigma^2_v), \\
\varepsilon_{ijk} \sim N(0, \sigma^2),$$

where $$y_{ijk}$$ is the observed yield of the $$i$$th genotype and $$j$$th fertilizer treatment in the $$k$$th block, 
$$\mu$$ is the overall mean, 
$$\tau_i$$ is the (fixed) effect of the $$i$$th genotype, 
$$\alpha_j$$ is the (fixed) effect of the $$j$$th fertilizer treatment, 
$$u_k$$ is the (random) effect of the $$k$$th block, 
$$v_{i \vert k}$$ is the (random) effect of the $$i$$th whole plot (genotype) in the$$k$$th block, 
$$\varepsilon_{ijk}$$ is the residual (i.e., the difference between predicted and observed) of the $$i$$th genotype and $$j$$th fertilizer treatment in the $$k$$th block, 
$$\sigma^2_u$$ is the variance among blocks, 
$$\sigma^2_v$$ is the variance among whole-plots, 
$$\sigma^2$$ is the residual variance. 

Now, the random effects are blocks and the whole plot (genotype)and thus, 
the matrix $$\mathbf{Z}$$ gets bigger:   

$$\mathbf{Z} = \begin{array}{cc}
\text{b}1 \ \ \text{b}2 \ \ \text{b}3 \ \ \text{b}4 \ \ \text{b}5 \ \ \text{b}6 \ \text{g}1\text{b}1 \ \text{g}2\text{b}1 \dots \text{g}2\text{b}6 \\ 
\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & \dots & 0\\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & \dots & 0\\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & 0\\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & \dots & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & \dots & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & 0\\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & \dots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & \dots & 1 \end{bmatrix}
\end{array}.$$


{% highlight r %}
dat <- yates.oats
dat$nf <- factor(dat$nitro)

m_splitplot <- glmmTMB(yield ~ gen*nf + (1|block/gen), data = dat)

summary(m_splitplot)
{% endhighlight %}



{% highlight text %}
##  Family: gaussian  ( identity )
## Formula:          yield ~ gen * nf + (1 | block/gen)
## Data: dat
## 
##      AIC      BIC   logLik deviance df.resid 
##    625.9    660.1   -298.0    595.9       57 
## 
## Random effects:
## 
## Conditional model:
##  Groups    Name        Variance Std.Dev.
##  gen:block (Intercept)  88.38    9.401  
##  block     (Intercept) 178.73   13.369  
##  Residual              147.57   12.148  
## Number of obs: 72, groups:  gen:block, 18; block, 6
## 
## Dispersion estimate for gaussian family (sigma^2):  148 
## 
## Conditional model:
##                     Estimate Std. Error z value Pr(>|z|)    
## (Intercept)          80.0000     8.3135   9.623  < 2e-16 ***
## genMarvellous         6.6667     8.8686   0.752  0.45222    
## genVictory           -8.5000     8.8686  -0.958  0.33784    
## nf0.2                18.5000     7.0135   2.638  0.00835 ** 
## nf0.4                34.6667     7.0135   4.943 7.70e-07 ***
## nf0.6                44.8333     7.0135   6.392 1.63e-10 ***
## genMarvellous:nf0.2   3.3333     9.9187   0.336  0.73682    
## genVictory:nf0.2     -0.3334     9.9187  -0.034  0.97319    
## genMarvellous:nf0.4  -4.1666     9.9187  -0.420  0.67443    
## genVictory:nf0.4      4.6667     9.9187   0.470  0.63800    
## genMarvellous:nf0.6  -4.6666     9.9187  -0.470  0.63800    
## genVictory:nf0.6      2.1667     9.9187   0.218  0.82708    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
{% endhighlight %}



{% highlight r %}
simulateResiduals(m_splitplot, plot = TRUE)
{% endhighlight %}

{% include figure.html img="day2/DHARMa_splitplot.png" alt="" caption="" width="80%" %}

{% highlight text %}
## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. 
##  
## Scaled residual values: 0.512 0.14 0.704 0.972 0.192 0.288 0.232 0.52 0.212 0.748 0.636 0.676 0.148 0.072 0.088 0.992 0.824 0.708 0.556 0.056 ...
{% endhighlight %}


{% highlight r %}
car::Anova(m_splitplot)
{% endhighlight %}



{% highlight text %}
## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: yield
##           Chisq Df Pr(>Chisq)    
## gen      3.5648  2     0.1682    
## nf     135.6683  3     <2e-16 ***
## gen:nf   2.1803  6     0.9024    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
{% endhighlight %}


{% highlight r %}
marginal_means_splitplot <- emmeans(m_splitplot, ~nf)
{% endhighlight %}



{% highlight text %}
## NOTE: Results may be misleading due to involvement in interactions
{% endhighlight %}



{% highlight r %}
marginal_means_splitplot <- emmeans(m_splitplot, ~gen:nf)
{% endhighlight %}



{% highlight r %}
cld(marginal_means_splitplot, 
    adjust = "sidak", 
    Letters = letters)
{% endhighlight %}



{% highlight text %}
##  gen        nf  emmean   SE df lower.CL upper.CL .group 
##  Victory    0     71.5 8.31 57     46.7     96.3  a     
##  GoldenRain 0     80.0 8.31 57     55.2    104.8  ab    
##  Marvellous 0     86.7 8.31 57     61.9    111.4  abc   
##  Victory    0.2   89.7 8.31 57     64.9    114.4  abcd  
##  GoldenRain 0.2   98.5 8.31 57     73.7    123.3  abcde 
##  Marvellous 0.2  108.5 8.31 57     83.7    133.3   bcdef
##  Victory    0.4  110.8 8.31 57     86.1    135.6   bcdef
##  GoldenRain 0.4  114.7 8.31 57     89.9    139.4    cdef
##  Marvellous 0.4  117.2 8.31 57     92.4    141.9     def
##  Victory    0.6  118.5 8.31 57     93.7    143.3      ef
##  GoldenRain 0.6  124.8 8.31 57    100.1    149.6       f
##  Marvellous 0.6  126.8 8.31 57    102.1    151.6      ef
## 
## Confidence level used: 0.95 
## Conf-level adjustment: sidak method for 12 estimates 
## P value adjustment: sidak method for 66 tests 
## significance level used: alpha = 0.05 
## NOTE: If two or more means share the same grouping symbol,
##       then we cannot show them to be different.
##       But we also did not show them to be the same.
{% endhighlight %}


## Applied example II -- repeated measures    

For this last example, one of our attendees kidly shared his data with us. 
The data were generated from an experiment that was studying the effect of different feed additives 
that might be potential replacements for antibiotics in swine production. 
A total of 300 pigs were used in this study, with dietary treatment applied to pen in a 
randomized complete block design. 
- Experimental unit: pen. 
- Observational unit: pig. 
3 pigs per pen were sampled on day 5, 10, and 17 and fecal sample was then placed in 
drying oven at 55 C for 48 hours to determine fecal dry matter. 

$$y_{ijk} = \mu + \tau_i + \alpha_j + (\tau \alpha)_{ij} + u_{ij} + v_{i \vert k} + \varepsilon_{ijk},$$

where $$y_{ijkl}$$ is the observed fecal dry matter for the $$i$$th feed treatment, in $$j$$th room ($$\sim$$ block) 
$$k$$th pen, and $$l$$th pig, 
$$\mu$$ is the overall mean, 
$$\tau_i$$ is the (fixed) effect of the $$i$$th feed treatment, 
$$\alpha_j$$ is the (fixed) effect of the $$j$$th day, 
$$(\tau \alpha)_{ij}$$ is the interaction between the $$i$$th feed treatment and the $$j$$th day, 
$$u_k$$ is the (random) effect of the $$k$$th room, 
$$v_{i \vert k}$$ is the (random) effect of the $$i$$th pen in the$$k$$th room, 
$$\varepsilon_{ijkl}$$ is the residual (i.e., the difference between predicted and observed) of the $$i$$th feed treatment, in $$j$$th room ($$\sim$$ block) 
$$k$$th pen, and $$l$$th pig, 
$$\sigma^2_u$$ is the variance among rooms, 
$$\sigma^2_v$$ is the variance among pens. 

Now, the residuals are not $$\varepsilon \sim N(0, \sigma^2)$$ like we used to write before. 
The residuals are not independent because we are not accounting for the repeated measures in time yet. 
Because we assume a first-order autoregressive structure, we say 

$$\mathbf{y}_{i k l} \sim N(\boldsymbol{\mu}, \Sigma_ikl), \\
\Sigma_{ikl} = \sigma^2 \begin{bmatrix} 1 & \rho & \rho^2 \\
\rho & 1 & \rho \\
\rho^2 & \rho & 1\end{bmatrix}.$$


{% highlight r %}
dd_fecal <- read.csv("data/fecal_dm.csv")
dd_fecal <- dd_fecal %>% 
  mutate(across(Pig:Day, ~as.factor(.)))

m_subsampling_repeated <- glmmTMB(dry_matter_perc ~ Trt * Day + ar1(1 + Day |Pig) + (1|Room/Pen),
                                  data = dd_fecal)

res <- simulateResiduals(m_subsampling_repeated, plot = TRUE)
{% endhighlight %}

{% include figure.html img="day2/DHARMa_repeated.png" alt="" caption="" width="80%" %}

{% highlight r %}
marginal_means_feces <- emmeans(m_subsampling_repeated, ~ Day|Trt)

cld(marginal_means_feces, 
    adjust = "sidak", 
    Letters = letters)
{% endhighlight %}



{% highlight text %}
## Day = 5:
##  Trt emmean   SE  df lower.CL upper.CL .group
##  A     19.8 1.18 517     16.7     23.0  a    
##  E     20.8 1.18 517     17.7     23.9  a    
##  D     21.4 1.18 517     18.3     24.6  a    
##  F     22.2 1.18 517     19.0     25.3  a    
##  C     22.4 1.18 517     19.3     25.5  a    
##  B     23.7 1.18 517     20.6     26.8  a    
## 
## Day = 10:
##  Trt emmean   SE  df lower.CL upper.CL .group
##  F     13.7 1.26 517     10.4     17.0  a    
##  D     15.3 1.26 517     12.0     18.6  a    
##  A     15.9 1.26 517     12.6     19.2  a    
##  C     16.3 1.26 517     13.0     19.6  a    
##  E     16.7 1.26 517     13.3     20.0  a    
##  B     23.6 1.26 517     20.2     26.9   b   
## 
## Day = 17:
##  Trt emmean   SE  df lower.CL upper.CL .group
##  F     18.2 1.24 517     14.9     21.5  a    
##  C     18.3 1.24 517     15.0     21.5  a    
##  A     18.6 1.24 517     15.3     21.8  a    
##  D     18.9 1.24 517     15.6     22.1  a    
##  E     19.1 1.24 517     15.8     22.4  a    
##  B     21.8 1.24 517     18.5     25.0  a    
## 
## Confidence level used: 0.95 
## Conf-level adjustment: sidak method for 6 estimates 
## P value adjustment: sidak method for 15 tests 
## significance level used: alpha = 0.05 
## NOTE: If two or more means share the same grouping symbol,
##       then we cannot show them to be different.
##       But we also did not show them to be the same.
{% endhighlight %}


------

## What's next  

- Monday, same time, same place.  
- Non-normal response: proportions, counts, successes.  

Any questions? E-mail me!  

